{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "b1=0 #bias unit 1\n",
    "b2=0 #bias unit 2\n",
    "\n",
    "def sigmoid(x):      # sigmoid function\n",
    "    return 1 /(1+(math.e**-x))\n",
    "\n",
    "def softmax(x):     #softmax function\n",
    "    l_exp = np.exp(x)\n",
    "    sm = l_exp/np.sum(l_exp, axis=0)\n",
    "    return sm\n",
    "    \n",
    "# input dataset with 3 features\n",
    "X = np.array([  [.35,.21,.33],\n",
    "            \t[.2,.4,.3],\n",
    "            \t[.4,.34,.5],\n",
    "            \t[.18,.21,16] ])\n",
    "len_X = len(X) # training set size\n",
    "input_dim = 3 # input layer dimensionality\n",
    "output_dim = 1 # output layer dimensionality\n",
    "hidden_units=4\n",
    "  \n",
    "np.random.seed(22)\n",
    "# create random weight vectors\n",
    "theta0 = 2*np.random.random((input_dim, hidden_units))\n",
    "theta1 = 2*np.random.random((hidden_units, output_dim))\n",
    "\n",
    "# forward propagation pass\n",
    "d1 = X.dot(theta0)+b1\n",
    "l1=sigmoid(d1)\n",
    "l2 = l1.dot(theta1)+b2\n",
    "#letâ€™s apply softmax to the output of the final layer\n",
    "output=softmax(l2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backpropagation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neuralnet accuracy:0.446475342875\n",
      "Neuralnet accuracy:0.625318046022\n",
      "Neuralnet accuracy:0.639273569828\n",
      "Neuralnet accuracy:0.646973337655\n",
      "Neuralnet accuracy:0.65355100446\n",
      "Neuralnet accuracy:0.660011060327\n",
      "Neuralnet accuracy:0.666798132376\n",
      "Neuralnet accuracy:0.674498860374\n",
      "Neuralnet accuracy:0.684028475313\n",
      "Neuralnet accuracy:0.696299738928\n",
      "Neuralnet accuracy:0.711746799387\n",
      "Neuralnet accuracy:0.730046564033\n",
      "Neuralnet accuracy:0.750090593059\n",
      "Neuralnet accuracy:0.770419774119\n",
      "Neuralnet accuracy:0.789971860781\n",
      "Neuralnet accuracy:0.808263092189\n",
      "Neuralnet accuracy:0.824566058728\n",
      "Neuralnet accuracy:0.838737531167\n",
      "Neuralnet accuracy:0.851026112067\n",
      "Neuralnet accuracy:0.861691314965\n",
      "Neuralnet accuracy:0.870967069843\n",
      "Neuralnet accuracy:0.87906528131\n",
      "Neuralnet accuracy:0.886172293348\n",
      "Neuralnet accuracy:0.892445776114\n",
      "Neuralnet accuracy:0.898016226232\n",
      "Neuralnet accuracy:0.902990776676\n",
      "Neuralnet accuracy:0.907457318163\n",
      "Neuralnet accuracy:0.9114881652\n",
      "Neuralnet accuracy:0.915143094558\n",
      "Neuralnet accuracy:0.918471791299\n",
      "Neuralnet accuracy:0.921515794356\n",
      "Neuralnet accuracy:0.924310038077\n",
      "Neuralnet accuracy:0.926884074152\n",
      "Neuralnet accuracy:0.929263042912\n",
      "Neuralnet accuracy:0.931468448618\n",
      "Neuralnet accuracy:0.933518781357\n",
      "Neuralnet accuracy:0.935430018544\n",
      "Neuralnet accuracy:0.93721603157\n",
      "Neuralnet accuracy:0.938888917354\n",
      "Neuralnet accuracy:0.940459270157\n",
      "Neuralnet accuracy:0.941936405605\n",
      "Neuralnet accuracy:0.943328546288\n",
      "Neuralnet accuracy:0.9446429763\n",
      "Neuralnet accuracy:0.94588617054\n",
      "Neuralnet accuracy:0.947063903396\n",
      "Neuralnet accuracy:0.948181340512\n",
      "Neuralnet accuracy:0.949243116608\n",
      "Neuralnet accuracy:0.950253401729\n",
      "Neuralnet accuracy:0.951215957882\n",
      "Neuralnet accuracy:0.952134187629\n",
      "Neuralnet accuracy:0.953011175929\n",
      "Neuralnet accuracy:0.953849726295\n",
      "Neuralnet accuracy:0.954652392145\n",
      "Neuralnet accuracy:0.95542150406\n",
      "Neuralnet accuracy:0.956159193575\n",
      "Neuralnet accuracy:0.956867413985\n",
      "Neuralnet accuracy:0.957547958608\n",
      "Neuralnet accuracy:0.958202476846\n",
      "Neuralnet accuracy:0.958832488361\n",
      "Neuralnet accuracy:0.959439395601\n",
      "Neuralnet accuracy:0.960024494914\n",
      "Neuralnet accuracy:0.960588986414\n",
      "Neuralnet accuracy:0.961133982771\n",
      "Neuralnet accuracy:0.961660517057\n",
      "Neuralnet accuracy:0.962169549758\n",
      "Neuralnet accuracy:0.962661975062\n",
      "Neuralnet accuracy:0.963138626503\n",
      "Neuralnet accuracy:0.963600282036\n",
      "Neuralnet accuracy:0.964047668613\n",
      "Neuralnet accuracy:0.964481466313\n",
      "Neuralnet accuracy:0.964902312079\n",
      "Neuralnet accuracy:0.965310803098\n",
      "Neuralnet accuracy:0.965707499872\n",
      "Neuralnet accuracy:0.966092929005\n",
      "Neuralnet accuracy:0.966467585744\n",
      "Neuralnet accuracy:0.96683193629\n",
      "Neuralnet accuracy:0.96718641991\n",
      "Neuralnet accuracy:0.967531450862\n",
      "Neuralnet accuracy:0.967867420166\n",
      "Neuralnet accuracy:0.968194697219\n",
      "Neuralnet accuracy:0.968513631276\n",
      "Neuralnet accuracy:0.96882455282\n",
      "Neuralnet accuracy:0.969127774808\n",
      "Neuralnet accuracy:0.969423593829\n",
      "Neuralnet accuracy:0.969712291169\n",
      "Neuralnet accuracy:0.96999413379\n",
      "Neuralnet accuracy:0.970269375238\n",
      "Neuralnet accuracy:0.970538256484\n",
      "Neuralnet accuracy:0.970801006696\n",
      "Neuralnet accuracy:0.97105784396\n",
      "Neuralnet accuracy:0.97130897595\n",
      "Neuralnet accuracy:0.971554600541\n",
      "Neuralnet accuracy:0.971794906393\n",
      "Neuralnet accuracy:0.972030073479\n",
      "Neuralnet accuracy:0.972260273585\n",
      "Neuralnet accuracy:0.972485670778\n",
      "Neuralnet accuracy:0.972706421835\n",
      "Neuralnet accuracy:0.972922676646\n",
      "Neuralnet accuracy:0.973134578595\n",
      "Neuralnet accuracy:0.97334226491\n",
      "Neuralnet accuracy:0.973545866994\n",
      "Neuralnet accuracy:0.973745510734\n",
      "Neuralnet accuracy:0.973941316788\n",
      "Neuralnet accuracy:0.974133400859\n",
      "Neuralnet accuracy:0.974321873946\n",
      "Neuralnet accuracy:0.974506842588\n",
      "Neuralnet accuracy:0.974688409082\n",
      "Neuralnet accuracy:0.974866671698\n",
      "Neuralnet accuracy:0.975041724877\n",
      "Neuralnet accuracy:0.975213659417\n",
      "Neuralnet accuracy:0.975382562647\n",
      "Neuralnet accuracy:0.975548518596\n",
      "Neuralnet accuracy:0.975711608147\n",
      "Neuralnet accuracy:0.975871909184\n",
      "Neuralnet accuracy:0.976029496733\n",
      "Neuralnet accuracy:0.976184443092\n",
      "Neuralnet accuracy:0.976336817956\n",
      "Neuralnet accuracy:0.976486688534\n",
      "Neuralnet accuracy:0.976634119659\n",
      "Neuralnet accuracy:0.976779173894\n",
      "Neuralnet accuracy:0.976921911635\n",
      "Neuralnet accuracy:0.977062391198\n",
      "Neuralnet accuracy:0.977200668914\n",
      "Neuralnet accuracy:0.977336799213\n",
      "Neuralnet accuracy:0.977470834704\n",
      "Neuralnet accuracy:0.977602826249\n",
      "Neuralnet accuracy:0.977732823042\n",
      "Neuralnet accuracy:0.977860872668\n",
      "Neuralnet accuracy:0.97798702118\n",
      "Neuralnet accuracy:0.978111313153\n",
      "Neuralnet accuracy:0.978233791744\n",
      "Neuralnet accuracy:0.978354498755\n",
      "Neuralnet accuracy:0.978473474679\n",
      "Neuralnet accuracy:0.978590758754\n",
      "Neuralnet accuracy:0.978706389015\n",
      "Neuralnet accuracy:0.978820402334\n",
      "Neuralnet accuracy:0.978932834472\n",
      "Neuralnet accuracy:0.979043720115\n",
      "Neuralnet accuracy:0.979153092916\n",
      "Neuralnet accuracy:0.979260985537\n",
      "Neuralnet accuracy:0.979367429682\n",
      "Neuralnet accuracy:0.979472456134\n",
      "Neuralnet accuracy:0.979576094786\n",
      "Neuralnet accuracy:0.979678374679\n",
      "Neuralnet accuracy:0.979779324025\n",
      "Neuralnet accuracy:0.979878970244\n",
      "Neuralnet accuracy:0.979977339987\n",
      "Neuralnet accuracy:0.980074459165\n",
      "Neuralnet accuracy:0.980170352976\n",
      "Neuralnet accuracy:0.980265045925\n",
      "Neuralnet accuracy:0.980358561856\n",
      "Neuralnet accuracy:0.980450923965\n",
      "Neuralnet accuracy:0.980542154829\n",
      "Neuralnet accuracy:0.980632276425\n",
      "Neuralnet accuracy:0.98072131015\n",
      "Neuralnet accuracy:0.980809276838\n",
      "Neuralnet accuracy:0.980896196782\n",
      "Neuralnet accuracy:0.980982089751\n",
      "Neuralnet accuracy:0.981066975004\n",
      "Neuralnet accuracy:0.981150871311\n",
      "Neuralnet accuracy:0.981233796965\n",
      "Neuralnet accuracy:0.9813157698\n",
      "Neuralnet accuracy:0.981396807203\n",
      "Neuralnet accuracy:0.981476926131\n",
      "Neuralnet accuracy:0.981556143121\n",
      "Neuralnet accuracy:0.981634474307\n",
      "Neuralnet accuracy:0.981711935427\n",
      "Neuralnet accuracy:0.981788541842\n",
      "Neuralnet accuracy:0.981864308541\n",
      "Neuralnet accuracy:0.981939250157\n",
      "Neuralnet accuracy:0.982013380975\n",
      "Neuralnet accuracy:0.982086714943\n",
      "Neuralnet accuracy:0.982159265684\n",
      "Neuralnet accuracy:0.982231046502\n",
      "Neuralnet accuracy:0.982302070394\n",
      "Neuralnet accuracy:0.982372350058\n",
      "Neuralnet accuracy:0.982441897902\n",
      "Neuralnet accuracy:0.982510726052\n",
      "Neuralnet accuracy:0.98257884636\n",
      "Neuralnet accuracy:0.982646270413\n",
      "Neuralnet accuracy:0.982713009539\n",
      "Neuralnet accuracy:0.982779074814\n",
      "Neuralnet accuracy:0.982844477073\n",
      "Neuralnet accuracy:0.982909226908\n",
      "Neuralnet accuracy:0.982973334687\n",
      "Neuralnet accuracy:0.983036810548\n",
      "Neuralnet accuracy:0.983099664413\n",
      "Neuralnet accuracy:0.983161905993\n",
      "Neuralnet accuracy:0.983223544789\n",
      "Neuralnet accuracy:0.983284590104\n",
      "Neuralnet accuracy:0.983345051044\n",
      "Neuralnet accuracy:0.983404936523\n",
      "Neuralnet accuracy:0.983464255273\n",
      "Neuralnet accuracy:0.983523015841\n",
      "Neuralnet accuracy:0.983581226603\n",
      "Neuralnet accuracy:0.983638895759\n",
      "Neuralnet accuracy:0.983696031345\n",
      "Neuralnet accuracy:0.983752641234\n",
      "Neuralnet accuracy:0.983808733139\n",
      "Neuralnet accuracy:0.98386431462\n",
      "Neuralnet accuracy:0.983919393086\n",
      "Neuralnet accuracy:0.983973975799\n",
      "Neuralnet accuracy:0.984028069878\n",
      "Neuralnet accuracy:0.984081682304\n",
      "Neuralnet accuracy:0.984134819919\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "def sigmoid(x):      # sigmoid function\n",
    "\treturn 1 /(1+(math.e**-x))\n",
    "\n",
    "def deriv_sigmoid(y): #the derivative of the sigmoid function\n",
    "    return y * (1.0 - y)   \n",
    "    \n",
    "alpha=.1    #this is the learning rate\n",
    "X = np.array([  [.35,.21,.33],\n",
    "            \t[.2,.4,.3],\n",
    "            \t[.4,.34,.5],\n",
    "            \t[.18,.21,16] ])                \n",
    "y = np.array([[0],\n",
    "\t\t[1],\n",
    "\t\t[1],\n",
    "\t\t[0]])\n",
    "np.random.seed(1)\n",
    "#We randomly initialize the layers\n",
    "theta0 = 2*np.random.random((3,4)) - 1\n",
    "theta1 = 2*np.random.random((4,1)) - 1\n",
    "\n",
    "for iter in range(205000): #here we specify the amount of training rounds.\n",
    "\t# Feedforward the input like we did in the previous exercise\n",
    "    input_layer = X\n",
    "    l1 = sigmoid(np.dot(input_layer,theta0))\n",
    "    l2 = sigmoid(np.dot(l1,theta1))\n",
    "\n",
    "    # Calculate error \n",
    "    l2_error = y - l2\n",
    "    \n",
    "    if (iter% 1000) == 0:\n",
    "        print(\"Neuralnet accuracy:\" + str(np.mean(1-(np.abs(l2_error)))))\n",
    "        \n",
    "    # Calculate the gradients in vectorized form \n",
    "    # Softmax and bias units are left out for instructional simplicity\n",
    "    l2_delta = alpha*(l2_error*deriv_sigmoid(l2))\n",
    "    l1_error = l2_delta.dot(theta1.T)\n",
    "    l1_delta = alpha*(l1_error * deriv_sigmoid(l1))\n",
    "\n",
    "    theta1 += l1.T.dot(l2_delta)\n",
    "    theta0 += input_layer.T.dot(l1_delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What & how Neural Networks Learn\n",
    "(An example with Neurolab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'neurolab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-19aa533dbf29>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mneurolab\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstyle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ggplot'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'neurolab'"
     ]
    }
   ],
   "source": [
    "import neurolab as nl\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "# Create train samples\n",
    "x = np.linspace(-10,10, 60)\n",
    "y = np.cos(x) * 0.9\n",
    "size = len(x)\n",
    "x_train = x.reshape(size,1)\n",
    "y_train = y.reshape(size,1)\n",
    "\n",
    "# Create network with 4 layers and random initialized\n",
    "# just experiment with the amount of layers\n",
    "\n",
    "d=[[1,1],[45,1],[45,45,1],[45,45,45,1]]\n",
    "for i in range(4):\n",
    "    net = nl.net.newff([[-10, 10]],d[i])\n",
    "    train_net=nl.train.train_gd(net, x_train, y_train, epochs=1000, show=100)\n",
    "    outp=net.sim(x_train)\n",
    "# Plot results (dual plot with error curve and predicted values)\n",
    "    import matplotlib.pyplot \n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(train_net)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('squared error')\n",
    "    x2 = np.linspace(-10.0,10.0,150)\n",
    "    y2 = net.sim(x2.reshape(x2.size,1)).reshape(x2.size)\n",
    "    y3 = outp.reshape(size)\n",
    "    plt.subplot(2, 1, 2)\n",
    "\n",
    "    plt.suptitle([i ,'hidden layers'])\n",
    "    plt.plot(x2, y2, '-',x , y, '.', x, y3, 'p')\n",
    "    plt.legend(['y predicted', 'y_target'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks with SKNN in Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sknn.mlp import Classifier, Layer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import cross_validation\n",
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data,\n",
    "iris.target, test_size=0.2, random_state=0)\n",
    "\n",
    "\n",
    "X_trainn = preprocessing.normalize(X_train, norm='l2')\n",
    "X_testn = preprocessing.normalize(X_test, norm='l2')\n",
    "\n",
    "X_trainn = preprocessing.scale(X_trainn)\n",
    "X_testn = preprocessing.scale(X_testn)\n",
    "\n",
    "clf = Classifier(\n",
    "\tlayers=[\n",
    "    \tLayer(\"Rectifier\", units=13),   \n",
    "    \tLayer(\"Rectifier\", units=13),   \n",
    "    \tLayer(\"Softmax\")],    \tlearning_rate=0.001, learning_rule='sgd',random_state=201,\n",
    "\tn_iter=200)\n",
    "\n",
    "model1=clf.fit(X_trainn, y_train)\n",
    "y_hat=clf.predict(X_testn)\n",
    "scores = cross_validation.cross_val_score(clf, X_trainn, y_train, cv=5)\n",
    "print('train mean accuracy %s' % np.mean(scores))\n",
    "print('vanilla sgd test %s' % accuracy_score(y_hat,y_test))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesterov momentum "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = Classifier(\n",
    "\tlayers=[\n",
    "    \tLayer(\"Rectifier\", units=13),   \n",
    "    \tLayer(\"Rectifier\", units=13),   \n",
    "    \tLayer(\"Softmax\")],    \tlearning_rate=0.001,learning_rule='nesterov',random_state=101,\n",
    "\tn_iter=1000)\n",
    "\n",
    "model1=clf.fit(X_trainn, y_train)\n",
    "y_hat=clf.predict(X_testn)\n",
    "scores = cross_validation.cross_val_score(clf, X_trainn, y_train, cv=5)\n",
    "print('Nesterov train mean accuracy %s' % np.mean(scores))\n",
    "print('Nesterov  test %s' % accuracy_score(y_hat,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = Classifier(\n",
    "\tlayers=[\n",
    "    \tLayer(\"Rectifier\", units=13),\n",
    "    \tLayer(\"Rectifier\", units=13),\n",
    "    \tLayer(\"Softmax\")],\n",
    "\tlearning_rate=0.01,\n",
    "\tn_iter=2000,\n",
    "\tlearning_rule='nesterov',\n",
    "\tregularize='dropout', #here we specify dropout\n",
    "\tdropout_rate=.1,#dropout fraction of neural units in entire network\n",
    "\trandom_state=0)\n",
    "model1=clf.fit(X_trainn, y_train)\n",
    "\n",
    "scores = cross_validation.cross_val_score(clf, X_trainn, y_train, cv=5)\n",
    "print(np.mean(scores))\n",
    "y_hat=clf.predict(X_testn)\n",
    "print(accuracy_score(y_hat,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Learning & Hyper parameter optimization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp \n",
    "import pandas as pd\n",
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "from sklearn.grid_search import GridSearchCV, RandomizedSearchCV\n",
    "from scipy import stats\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sknn.mlp import  Layer, Regressor, Classifier as skClassifier\n",
    "\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv', sep = ';')\n",
    "X = df.drop('quality', 1).values # drop target variable\n",
    "   \n",
    "\n",
    "y1 = df['quality'].values # original target variable\n",
    "y = y1 <= 5 # new target variable: is the rating <= 5?\n",
    "\n",
    "# Split the data into a test set and a training set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(X_train.shape)\n",
    "\n",
    "\n",
    "max_net = skClassifier(layers= [Layer(\"Rectifier\",units=1),\n",
    "                                       Layer(\"Rectifier\",units=1),\n",
    "                                       Layer(\"Rectifier\",units=1),\n",
    "                                       Layer(\"Softmax\")])\n",
    "params={'learning_rate': [.002],\n",
    "        'hidden0__units': sp.stats.randint(8, 20),\n",
    "        'hidden0__type': [\"Rectifier\"],\n",
    "        'hidden1__units': sp.stats.randint(8, 20),\n",
    "        'hidden1__type': [\"Rectifier\"],\n",
    "        'learning_rule':[\"adam\",\"rmsprop\",\"sgd\"]}\n",
    "max_net2 = RandomizedSearchCV(max_net,param_distributions=params,n_iter=10,cv=3,random_state=101,scoring='accuracy',verbose=10,\\\n",
    "                             pre_dispatch=None)\n",
    "model_tuning=max_net2.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "\n",
    "print \"best score %s\" % model_tuning.best_score_\n",
    "print \"best parameters %s\" % model_tuning.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Networks & Decision Boundaries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "%matplotlib inline\n",
    "from sknn.mlp import Classifier, Layer\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from itertools import product\n",
    "\n",
    "\n",
    "X,y= datasets.make_moons(n_samples=500, noise=.2, random_state=222)\n",
    "from sklearn.datasets import make_blobs\n",
    " \n",
    "net1 = Classifier(\n",
    "   layers=[\n",
    "       Layer(\"Softmax\")],random_state=222,\n",
    "   learning_rate=0.01,\n",
    "   n_iter=100)\n",
    "net2 = Classifier(\n",
    "   layers=[\n",
    "       Layer(\"Rectifier\", units=4),\n",
    "       Layer(\"Softmax\")],random_state=12,\n",
    "   learning_rate=0.01,\n",
    "   n_iter=100)\n",
    "net3 =Classifier(\n",
    "   layers=[\n",
    "       Layer(\"Rectifier\", units=4),\n",
    "       Layer(\"Rectifier\", units=4),\n",
    "       Layer(\"Softmax\")],random_state=22,\n",
    "   learning_rate=0.01,\n",
    "   n_iter=100)\n",
    "net4 =Classifier(\n",
    "   layers=[\n",
    "       Layer(\"Rectifier\", units=4),\n",
    "       Layer(\"Rectifier\", units=4),\n",
    "       Layer(\"Rectifier\", units=4),\n",
    "       Layer(\"Rectifier\", units=4),\n",
    "       Layer(\"Rectifier\", units=4),\n",
    "       Layer(\"Rectifier\", units=4),\n",
    "       Layer(\"Softmax\")],random_state=62,\n",
    "   learning_rate=0.01,\n",
    "   n_iter=100)\n",
    " \n",
    "net1.fit(X, y)\n",
    "net2.fit(X, y)\n",
    "net3.fit(X, y)\n",
    "net4.fit(X, y)\n",
    " \n",
    " \n",
    "# Plotting decision regions\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
    "                    np.arange(y_min, y_max, 0.1))\n",
    " \n",
    "f, arxxx = plt.subplots(2, 2, sharey='row',sharex='col', figsize=(8, 8))\n",
    "plt.suptitle('Neural Network - Decision Boundary')\n",
    "for idx, clf, ti in zip(product([0, 1], [0, 1]),\n",
    "                       [net1, net2, net3,net4],\n",
    "                       ['0 hidden layer', '1 hidden layer',\n",
    "                        '2 hidden layers','6 hidden layers']):\n",
    " \n",
    "   Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "   Z = Z.reshape(xx.shape)\n",
    "\n",
    "   arxxx[idx[0], idx[1]].contourf(xx, yy, Z, alpha=0.5)\n",
    "   arxxx[idx[0], idx[1]].scatter(X[:, 0], X[:, 1], c=y, alpha=0.5)\n",
    "   arxxx[idx[0], idx[1]].set_title(ti)\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Deep learning with H2o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys  \n",
    "import h2o  \n",
    "h2o.init() \n",
    "\n",
    "h2o.cluster_info()\n",
    "\n",
    "train_url =\"https://h2o-public-test-data.s3.amazonaws.com/bigdata/laptop/mnist/train.csv.gz\"\n",
    "test_url=\"https://h2o-public-test-data.s3.amazonaws.com/bigdata/laptop/mnist/test.csv.gz\"\n",
    "\n",
    "\n",
    "train=h2o.import_file(train_url)\n",
    "test=h2o.import_file(test_url)\n",
    "\n",
    "train.describe()\n",
    "test.describe()\n",
    "\n",
    "\n",
    "y='C785'\n",
    "x=train.names[0:784]\n",
    "train[y]=train[y].asfactor()\n",
    "test[y]=test[y].asfactor()\n",
    "\n",
    "from h2o.estimators.deeplearning import H2ODeepLearningEstimator\n",
    "\n",
    "model_cv=H2ODeepLearningEstimator(distribution='multinomial'\n",
    "                                 ,activation='RectifierWithDropout',hidden=[32,32,32],\n",
    "                                        input_dropout_ratio=.2,\n",
    "                                        sparse=True,\n",
    "                                        l1=.0005,\n",
    "                                            epochs=5)\n",
    "                                            \n",
    "model_cv.train(x=x,y=y,training_frame=train,nfolds=3)\n",
    "print model_cv\n",
    "\n",
    "print model_cv.scoring_history()\n",
    "\n",
    "\n",
    "model_cv.train(x=x,y=y,training_frame=train,validation_frame=test,nfolds=3)\n",
    "print model_cv \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "h2o and gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#h2o & gridsearch\n",
    "hidden_opt = [[18,18],[32,32],[32,32,32],[100,100,100]]\n",
    "hyper_parameters = {\"hidden\":hidden_opt}\n",
    "\n",
    "#important: here we specify the search parameters\n",
    "#be careful with these, the training time can explode (see max_models)\n",
    "search_c = {\"strategy\":\"RandomDiscrete\",\n",
    "\n",
    "\"max_models\":10, \"max_runtime_secs\":100,\n",
    "\n",
    "\"seed\":222}\n",
    "\n",
    "\n",
    "from h2o.grid.grid_search import H2OGridSearch\n",
    "\n",
    "model_grid = H2OGridSearch(H2ODeepLearningEstimator, hyper_params=hyper_parameters)\n",
    "\n",
    "model_grid.train(x=x, y=y, distribution=\"multinomial\", epochs=1000, training_frame=train, validation_frame=test,\n",
    "\tscore_interval=2, stopping_rounds=3, stopping_tolerance=0.05,search_criteria=search_c)\n",
    "\n",
    "print model_grid\n",
    "\n",
    "h2o.shutdown(prompt=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning and unsupervised pretraining (THEANETS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks with Theanets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "import climate # This package provides the reporting of iterations \n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import theanets\n",
    "import theano\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import climate\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import theanets\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import datasets\n",
    "climate.enable_default_logging()\n",
    "\n",
    "digits = datasets.load_digits()\n",
    "digits = datasets.load_digits()\n",
    "X = np.asarray(digits.data, 'float32')\n",
    "\n",
    "Y = digits.target\n",
    "\n",
    "Y=np.array(Y, dtype=np.int32)\n",
    "#X = (X - np.min(X, 0)) / (np.max(X, 0) + 0.0001)  # 0-1 scaling\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=0)\n",
    "\n",
    "\n",
    "\n",
    "# Build a classifier model with 64 inputs, 1 hidden layer with 100 units  and 10 outputs.\n",
    "net = theanets.Classifier([64,100,10])\n",
    "N=10\n",
    "# Train the model using Resilient backpropagation and momentum.\n",
    "net.train([X_train,y_train], algo='sgd', learning_rate=.001, momentum=0.9,patience=0,\n",
    "validate_every=N,\n",
    "min_improvement=0.8)\n",
    "\n",
    "# Show confusion matrices on the training/validation splits.\n",
    "print(confusion_matrix(y_test, net.predict(X_test)))\n",
    "print (accuracy_score(y_test, net.predict(X_test)))\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autoencoders "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = theanets.Autoencoder([64,(45,'relu'),(45,'relu'),(45,'relu'),(45,'relu'),(45,'relu'),64])\n",
    "dAE_model=model.train([X_train],algo='rmsprop',input_noise=0.1,hidden_l1=.001,sparsity=0.9,num_updates=1000)\n",
    "X_dAE=model.encode(X_train)\n",
    "X_dAE=np.asarray(X_dAE, 'float32')\n",
    "X_dAE.shape\n",
    "\n",
    "\n",
    "net = theanets.Classifier(layers=(45,45,45,10))\n",
    "autoe=net.train([X_dAE, y_train], algo='rmsprop',learning_rate=.0001,batch_size=110,min_improvement=.8,momentum=.9,\n",
    "nesterov=True,num_updates=1000)\n",
    "\n",
    "dAE_model=model.train([X_test],algo='rmsprop',input_noise=0.1,hidden_l1=.001,sparsity=0.9,momentum=0.9,patience=0,validate_every=N,\n",
    "min_improvement=0.8)\n",
    "X_dAE2=model.encode(X_test)\n",
    "X_dAE2=np.asarray(X_dAE2, 'float32')\n",
    "\n",
    "#now watch how the higher accuracy of the neural network WITH autoencoders (.975 vs .961).\n",
    "final=net.predict(X_dAE2)\n",
    "from sklearn.metrics import accuracy_score\n",
    "print accuracy_score(final,y_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
