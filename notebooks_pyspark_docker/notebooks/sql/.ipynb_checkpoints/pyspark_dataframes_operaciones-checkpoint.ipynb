{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### PySpark alias function(del tutorial [pyspark.sql.DataFrame.alias](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.alias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "+-----+\n",
      "|   to|\n",
      "+-----+\n",
      "|  Bob|\n",
      "|Carol|\n",
      "| Dave|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# alias\n",
    "from pyspark.sql.functions import col\n",
    "x = sqlContext.createDataFrame([(\"Alice\",\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = x.alias('transactions')\n",
    "x.show()\n",
    "y.show()\n",
    "y.select(col(\"transactions.to\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### PySpark collect function(del tutorial [pyspark.sql.DataFrame.collect](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.collect))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "[Row(from=u'Alice', to=u'Bob', amt=0.1), Row(from=u'Bob', to=u'Carol', amt=0.2), Row(from=u'Carol', to=u'Dave', amt=0.3)]\n"
     ]
    }
   ],
   "source": [
    "# collect\n",
    "# crea una lista de objetos Row\n",
    "x = sqlContext.createDataFrame([(\"Alice\",\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = x.collect()\n",
    "x.show()\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### PySpark columns function(del tutorial [pyspark.sql.DataFrame.columns](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "['from', 'to', 'amt']\n"
     ]
    }
   ],
   "source": [
    "# columns\n",
    "#crea una lista de nombres de columnas\n",
    "x = sqlContext.createDataFrame([(\"Alice\",\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = x.columns \n",
    "x.show()\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### PySpark count function(del tutorial [pyspark.sql.DataFrame.count](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# count\n",
    "# contador de filas\n",
    "x = sqlContext.createDataFrame([(\"Alice\",\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "x.show()\n",
    "print(x.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### PySpark crosstab function(del tutorial [pyspark.sql.DataFrame.crosstab](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.crosstab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "|Carol| Dave|0.4|\n",
      "+-----+-----+---+\n",
      "\n",
      "+-------+---+-----+----+\n",
      "|from_to|Bob|Carol|Dave|\n",
      "+-------+---+-----+----+\n",
      "|    Bob|  0|    1|   0|\n",
      "|  Alice|  1|    0|   0|\n",
      "|  Carol|  0|    0|   2|\n",
      "+-------+---+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# crosstab\n",
    "# tabla de frecuencias\n",
    "x = sqlContext.createDataFrame([(\"Alice\",\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3),(\"Carol\",\"Dave\",0.4)], ['from','to','amt'])\n",
    "y = x.crosstab(col1='from',col2='to')\n",
    "x.show()\n",
    "y.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### PySpark describe function(del tutorial [pyspark.sql.DataFrame.describe](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.describe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "+-------+-----+----+-------------------+\n",
      "|summary| from|  to|                amt|\n",
      "+-------+-----+----+-------------------+\n",
      "|  count|    3|   3|                  3|\n",
      "|   mean| null|null|0.20000000000000004|\n",
      "| stddev| null|null|0.09999999999999998|\n",
      "|    min|Alice| Bob|                0.1|\n",
      "|    max|Carol|Dave|                0.3|\n",
      "+-------+-----+----+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# describe\n",
    "# obtener m√°s detalles de la tabla\n",
    "x = sqlContext.createDataFrame([(\"Alice\",\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "x.show()\n",
    "x.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### PySpark distinct function(del tutorial [pyspark.sql.DataFrame.distinct](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.distinct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "|  Bob|Carol|0.2|\n",
      "+-----+-----+---+\n",
      "\n",
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|  Bob|Carol|0.2|\n",
      "|Alice|  Bob|0.1|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# distinct\n",
    "# eliminar elementos o filas duplicadas\n",
    "x = sqlContext.createDataFrame([(\"Alice\",\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3),(\"Bob\",\"Carol\",0.2)], ['from','to','amt'])\n",
    "y = x.distinct()\n",
    "x.show()\n",
    "y.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### PySpark drop function(del tutorial [pyspark.sql.DataFrame.drop](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.drop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "+-----+-----+\n",
      "| from|   to|\n",
      "+-----+-----+\n",
      "|Alice|  Bob|\n",
      "|  Bob|Carol|\n",
      "|Carol| Dave|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# drop\n",
    "# eliminar columnas de los resultados\n",
    "x = sqlContext.createDataFrame([(\"Alice\",\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = x.drop('amt')\n",
    "x.show()\n",
    "y.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### PySpark dropDuplicates function(del tutorial [pyspark.sql.DataFrame.dropDuplicates](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.dropDuplicates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|  Bob|Carol|0.3|\n",
      "|  Bob|Carol|0.2|\n",
      "+-----+-----+---+\n",
      "\n",
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|  Bob|Carol|0.2|\n",
      "|Alice|  Bob|0.1|\n",
      "+-----+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# dropDuplicates\n",
    "# eliminar valores duplicados a partir de las columnas indicadas en subset\n",
    "x = sqlContext.createDataFrame([(\"Alice\",\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Bob\",\"Carol\",0.3),(\"Bob\",\"Carol\",0.2)], ['from','to','amt'])\n",
    "y = x.dropDuplicates(subset=['from','to'])\n",
    "x.show()\n",
    "y.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "PySpark dropna function(del tutorial [pyspark.sql.DataFrame.dropna](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.dropna))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+----+\n",
      "| from|   to| amt|\n",
      "+-----+-----+----+\n",
      "| null|  Bob| 0.1|\n",
      "|  Bob|Carol|null|\n",
      "|Carol| null| 0.3|\n",
      "|  Bob|Carol| 0.2|\n",
      "+-----+-----+----+\n",
      "\n",
      "+----+-----+----+\n",
      "|from|   to| amt|\n",
      "+----+-----+----+\n",
      "| Bob|Carol|null|\n",
      "| Bob|Carol| 0.2|\n",
      "+----+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# dropna\n",
    "# eliminar valores nulos(None)\n",
    "x = sqlContext.createDataFrame([(None,\"Bob\",0.1),(\"Bob\",\"Carol\",None),(\"Carol\",None,0.3),(\"Bob\",\"Carol\",0.2)], ['from','to','amt'])\n",
    "y = x.dropna(how='any',subset=['from','to'])\n",
    "x.show()\n",
    "y.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "PySpark dtypes function(del tutorial [pyspark.sql.DataFrame.dtypes](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.dtypes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "[('from', 'string'), ('to', 'string'), ('amt', 'double')]\n"
     ]
    }
   ],
   "source": [
    "# dtypes\n",
    "# obtiene los tipos de datos de las columnas\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = x.dtypes\n",
    "x.show()\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "PySpark fillna function(del tutorial [pyspark.sql.DataFrame.fillna](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.fillna))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+----+\n",
      "| from|   to| amt|\n",
      "+-----+-----+----+\n",
      "| null|  Bob| 0.1|\n",
      "|  Bob|Carol|null|\n",
      "|Carol| null| 0.3|\n",
      "+-----+-----+----+\n",
      "\n",
      "+-------+-------+----+\n",
      "|   from|     to| amt|\n",
      "+-------+-------+----+\n",
      "|unknown|    Bob| 0.1|\n",
      "|    Bob|  Carol|null|\n",
      "|  Carol|unknown| 0.3|\n",
      "+-------+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# fillna\n",
    "# reemplaza valores nulos por el valor que tu le indiques\n",
    "x = sqlContext.createDataFrame([(None,\"Bob\",0.1),(\"Bob\",\"Carol\",None),(\"Carol\",None,0.3)], ['from','to','amt'])\n",
    "y = x.fillna(value='unknown',subset=['from','to'])\n",
    "x.show()\n",
    "y.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "PySpark filter function(del tutorial [pyspark.sql.DataFrame.filter](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.filter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "+-----+----+---+\n",
      "| from|  to|amt|\n",
      "+-----+----+---+\n",
      "|Carol|Dave|0.3|\n",
      "+-----+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filter\n",
    "# filtra datos en funci√≥n de la condici√≥n indicada\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = x.filter(\"amt > 0.2\")\n",
    "x.show()\n",
    "y.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "PySpark first function(del tutorial [pyspark.sql.DataFrame.first](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.first))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "Row(from=u'Alice', to=u'Bob', amt=0.1)\n"
     ]
    }
   ],
   "source": [
    "# first\n",
    "# devuelve la primera fila\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = x.first()\n",
    "x.show()\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "PySpark foreach function(del tutorial [pyspark.sql.DataFrame.foreach](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.foreach))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "Row(from=u'Alice', to=u'Bob', amt=0.1)\n",
      "Row(from=u'Carol', to=u'Dave', amt=0.3)\n",
      "Row(from=u'Bob', to=u'Carol', amt=0.2)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# foreach\n",
    "# aplica una funci√≥n a todas las filas de la tabla\n",
    "from __future__ import print_function\n",
    "\n",
    "# setup\n",
    "fn = 'foreachExampleDataFrames.txt' \n",
    "open(fn, 'w').close()\n",
    "def writeFile(el,f):\n",
    "    '''appends el to file f'''\n",
    "    print(el,file=open(f, 'a+'))\n",
    "\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = x.foreach(lambda x: writeFile(x,fn)) # writes into foreachExampleDataFrames.txt\n",
    "x.show() # original dataframe\n",
    "# imprimir contenido del fichero\n",
    "with open(fn, \"r\") as foreachExample:\n",
    "    print (foreachExample.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "PySpark freqItems function(del tutorial [pyspark.sql.DataFrame.freqItems](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.freqItems))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|  Bob|Carol|0.1|\n",
      "|Alice| Dave|0.1|\n",
      "|Alice|  Bob|0.1|\n",
      "|Alice|  Bob|0.5|\n",
      "|Carol|  Bob|0.1|\n",
      "+-----+-----+---+\n",
      "\n",
      "+--------------+-------------+\n",
      "|from_freqItems|amt_freqItems|\n",
      "+--------------+-------------+\n",
      "|       [Alice]|        [0.1]|\n",
      "+--------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# freqItems\n",
    "x = sqlContext.createDataFrame([(\"Bob\",\"Carol\",0.1), (\"Alice\",\"Dave\",0.1), (\"Alice\",\"Bob\",0.1), (\"Alice\",\"Bob\",0.5), (\"Carol\",\"Bob\",0.1)], ['from','to','amt'])\n",
    "y = x.freqItems(cols=['from','amt'],support=0.8)\n",
    "x.show()\n",
    "y.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "PySpark groupBy function(del tutorial [pyspark.sql.DataFrame.groupBy](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.groupBy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|Alice|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "+-----+-------------------+\n",
      "| from|           avg(amt)|\n",
      "+-----+-------------------+\n",
      "|Carol|                0.3|\n",
      "|Alice|0.15000000000000002|\n",
      "+-----+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# groupBy(col1).avg(col2)\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Alice\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = x.groupBy('from').avg('amt')\n",
    "x.show()\n",
    "y.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "PySpark head function(del tutorial [pyspark.sql.DataFrame.head](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.head))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "[Row(from=u'Alice', to=u'Bob', amt=0.1)]\n"
     ]
    }
   ],
   "source": [
    "# head\n",
    "# devuelve las primeras n filas\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = x.head(1)\n",
    "x.show()\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "PySpark intersect function(del tutorial [pyspark.sql.DataFrame.intersect](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.intersect))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Alice|0.2|\n",
      "|Carol| Dave|0.1|\n",
      "+-----+-----+---+\n",
      "\n",
      "+-----+---+---+\n",
      "| from| to|amt|\n",
      "+-----+---+---+\n",
      "|Alice|Bob|0.1|\n",
      "+-----+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# intersect\n",
    "# devuelve un nuevo dataframe que contiene aquellas filas que se encuentran en los 2 dataframes\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Alice\",0.2),(\"Carol\",\"Dave\",0.1)], ['from','to','amt'])\n",
    "z = x.intersect(y)\n",
    "x.show()\n",
    "y.show()\n",
    "z.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "PySpark join function(del tutorial [pyspark.sql.DataFrame.join](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.join))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "+-----+---+\n",
      "| name|age|\n",
      "+-----+---+\n",
      "|Alice| 20|\n",
      "|  Bob| 40|\n",
      "| Dave| 80|\n",
      "+-----+---+\n",
      "\n",
      "+-----+----+---+---+\n",
      "| from|  to|amt|age|\n",
      "+-----+----+---+---+\n",
      "|Carol|Dave|0.3| 80|\n",
      "|Alice| Bob|0.1| 40|\n",
      "+-----+----+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# join\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = sqlContext.createDataFrame([('Alice',20),(\"Bob\",40),(\"Dave\",80)], ['name','age'])\n",
    "z = x.join(y,x.to == y.name,'inner').select('from','to','amt','age')\n",
    "x.show()\n",
    "y.show()\n",
    "z.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "PySpark limit function(del tutorial [pyspark.sql.DataFrame.limit](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.limit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "+-----+---+---+\n",
      "| from| to|amt|\n",
      "+-----+---+---+\n",
      "|Alice|Bob|0.1|\n",
      "+-----+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# limit\n",
    "# limitar el n√∫mero de resultados al n√∫mero indicado\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = x.limit(1)\n",
    "x.show()\n",
    "y.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "PySpark orderBy function(del tutorial [pyspark.sql.DataFrame.orderBy](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.orderBy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Carol| Dave|0.3|\n",
      "|  Bob|Carol|0.2|\n",
      "|Alice|  Bob|0.1|\n",
      "+-----+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# orderBy\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = x.orderBy(['amt'],ascending=[False])\n",
    "x.show()\n",
    "y.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "PySpark printSchema function(del tutorial [pyspark.sql.DataFrame.printSchema](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.printSchema))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "root\n",
      " |-- from: string (nullable = true)\n",
      " |-- to: string (nullable = true)\n",
      " |-- amt: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# printSchema\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "x.show()\n",
    "x.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "PySpark randomSplit function(del tutorial [pyspark.sql.DataFrame.randomSplit](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.randomSplit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "+-----+----+---+\n",
      "| from|  to|amt|\n",
      "+-----+----+---+\n",
      "|Carol|Dave|0.3|\n",
      "+-----+----+---+\n",
      "\n",
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "+-----+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# randomSplit\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = x.randomSplit([0.1,0.2])\n",
    "x.show()\n",
    "y[0].show()\n",
    "y[1].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "PySpark rdd function(del tutorial [pyspark.sql.DataFrame.rdd](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.rdd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "[Row(from=u'Alice', to=u'Bob', amt=0.1), Row(from=u'Bob', to=u'Carol', amt=0.2), Row(from=u'Carol', to=u'Dave', amt=0.3)]\n"
     ]
    }
   ],
   "source": [
    "# rdd\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = x.rdd\n",
    "x.show()\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "PySpark registerTempTable function(del tutorial [pyspark.sql.DataFrame.registerTempTable](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.registerTempTable))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# registerTempTable\n",
    "# registrar el dataframe como una tabla temporal sobre la cual se pueden realizar consultas sql\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "x.registerTempTable(name=\"TRANSACTIONS\")\n",
    "y = sqlContext.sql('SELECT * FROM TRANSACTIONS WHERE amt > 0.1')\n",
    "x.show()\n",
    "y.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "PySpark replace function(del tutorial [pyspark.sql.DataFrame.replace](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.replace))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol|David|0.3|\n",
      "+-----+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# replace\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = x.replace('Dave','David',['from','to'])\n",
    "x.show()\n",
    "y.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "StructType(List(StructField(from,StringType,true),StructField(to,StringType,true),StructField(amt,DoubleType,true)))\n"
     ]
    }
   ],
   "source": [
    "# schema\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = x.schema\n",
    "x.show()\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "PySpark select function(del tutorial [pyspark.sql.DataFrame.select](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.select))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "+-----+---+\n",
      "| from|amt|\n",
      "+-----+---+\n",
      "|Alice|0.1|\n",
      "|  Bob|0.2|\n",
      "|Carol|0.3|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = x.select(['from','amt'])\n",
    "x.show()\n",
    "y.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "PySpark selectExpr function(del tutorial [pyspark.sql.DataFrame.selectExpr](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.selectExpr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "+-----+-----+---------+\n",
      "| from|   to|(amt + 1)|\n",
      "+-----+-----+---------+\n",
      "|Alice|  Bob|      1.1|\n",
      "|  Bob|Carol|      1.2|\n",
      "|Carol| Dave|      1.3|\n",
      "+-----+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# selectExpr\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = x.selectExpr(['from','to','amt+1'])\n",
    "x.show()\n",
    "y.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "x.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "PySpark sort function(del tutorial [pyspark.sql.DataFrame.sort](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.sort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol|Alice|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Carol|Alice|0.3|\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "+-----+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sort\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Alice\",0.3)], ['from','to','amt'])\n",
    "y = x.sort(['to'])\n",
    "x.show()\n",
    "y.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "PySpark subtract function(del tutorial [pyspark.sql.DataFrame.subtract](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.subtract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.1|\n",
      "+-----+-----+---+\n",
      "\n",
      "+-----+----+---+\n",
      "| from|  to|amt|\n",
      "+-----+----+---+\n",
      "|Carol|Dave|0.3|\n",
      "+-----+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# subtract\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.1)], ['from','to','amt'])\n",
    "z = x.subtract(y)\n",
    "x.show()\n",
    "y.show()\n",
    "z.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "PySpark take function(del tutorial [pyspark.sql.DataFrame.take](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.take)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "[Row(from=u'Alice', to=u'Bob', amt=0.1), Row(from=u'Bob', to=u'Carol', amt=0.2)]\n"
     ]
    }
   ],
   "source": [
    "# take\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = x.take(num=2)\n",
    "x.show()\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "PySpark toDF function(del tutorial [pyspark.sql.DataFrame.toDF](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.toDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "+------+-----+------+\n",
      "|seller|buyer|amount|\n",
      "+------+-----+------+\n",
      "| Alice|  Bob|   0.1|\n",
      "|   Bob|Carol|   0.2|\n",
      "| Carol| Dave|   0.3|\n",
      "+------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# toDF\n",
    "# permite cambiar los nombres de las columnas\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = x.toDF(\"seller\",\"buyer\",\"amount\")\n",
    "x.show()\n",
    "y.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "PySpark toJSON function(del tutorial [pyspark.sql.DataFrame.toJSON](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.toJSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol|Alice|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "[u'{\"from\":\"Alice\",\"to\":\"Bob\",\"amt\":0.1}', u'{\"from\":\"Bob\",\"to\":\"Carol\",\"amt\":0.2}', u'{\"from\":\"Carol\",\"to\":\"Alice\",\"amt\":0.3}']\n"
     ]
    }
   ],
   "source": [
    "# toJSON\n",
    "# convierte un dataframe en una cadena json\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Alice\",0.3)], ['from','to','amt'])\n",
    "y = x.toJSON()\n",
    "x.show()\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "PySpark toPandas function(del tutorial [pyspark.sql.DataFrame.toPandas](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.toPandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>amt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alice</td>\n",
       "      <td>Bob</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bob</td>\n",
       "      <td>Carol</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Carol</td>\n",
       "      <td>Dave</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    from     to  amt\n",
       "0  Alice    Bob  0.1\n",
       "1    Bob  Carol  0.2\n",
       "2  Carol   Dave  0.3"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# toPandas\n",
    "# convierte un dataframe en un objeto pandas\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = x.toPandas()\n",
    "x.show()\n",
    "print(type(y))\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "PySpark unionAll function(del tutorial [pyspark.sql.DataFrame.unionAll](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.unionAll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "+-----+-----+---+\n",
      "\n",
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.1|\n",
      "+-----+-----+---+\n",
      "\n",
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.1|\n",
      "+-----+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# unionAll\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2)], ['from','to','amt'])\n",
    "y = sqlContext.createDataFrame([(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.1)], ['from','to','amt'])\n",
    "z = x.unionAll(y)\n",
    "x.show()\n",
    "y.show()\n",
    "z.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "PySpark where function(del tutorial [pyspark.sql.DataFrame.where](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.where)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# where (filter)\n",
    "# equivalente al filter\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = x.where(\"amt > 0.1\")\n",
    "x.show()\n",
    "y.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "PySpark withColumn function(del tutorial [pyspark.sql.DataFrame.withColumn](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.withColumn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+----+\n",
      "| from|   to| amt|\n",
      "+-----+-----+----+\n",
      "|Alice|  Bob| 0.1|\n",
      "|  Bob|Carol|null|\n",
      "|Carol| Dave| 0.3|\n",
      "+-----+-----+----+\n",
      "\n",
      "+-----+-----+----+-----+\n",
      "| from|   to| amt| conf|\n",
      "+-----+-----+----+-----+\n",
      "|Alice|  Bob| 0.1| true|\n",
      "|  Bob|Carol|null|false|\n",
      "|Carol| Dave| 0.3| true|\n",
      "+-----+-----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# withColumn\n",
    "# permite a√±adir una columna al dataframe\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",None),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = x.withColumn('conf',x.amt.isNotNull())\n",
    "x.show()\n",
    "y.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "PySpark withColumnRenamed function(del tutorial [pyspark.sql.DataFrame.withColumnRenamed](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.withColumnRenamed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "+-----+-----+------+\n",
      "| from|   to|amount|\n",
      "+-----+-----+------+\n",
      "|Alice|  Bob|   0.1|\n",
      "|  Bob|Carol|   0.2|\n",
      "|Carol| Dave|   0.3|\n",
      "+-----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# withColumnRenamed\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = x.withColumnRenamed('amt','amount')\n",
    "x.show()\n",
    "y.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "PySpark write function(del tutorial [pyspark.sql.DataFrame.write](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "+---+-----+-----+\n",
      "|amt| from|   to|\n",
      "+---+-----+-----+\n",
      "|0.3|Carol| Dave|\n",
      "|0.1|Alice|  Bob|\n",
      "|0.2|  Bob|Carol|\n",
      "+---+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# write\n",
    "import json\n",
    "x = sqlContext.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = x.write.mode('overwrite').json('dataframeWriteExample.json')\n",
    "x.show()\n",
    "# read the dataframe back in from file\n",
    "sqlContext.read.json('dataframeWriteExample.json').show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
